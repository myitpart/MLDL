{"cells":[{"cell_type":"markdown","id":"d61ee166-a2f7-4de2-a7ef-aa12086fd0a3","metadata":{"id":"d61ee166-a2f7-4de2-a7ef-aa12086fd0a3"},"source":["# Use segmentation\n","(see local folder - segmentation)\n","model for img 0-255, no normalization"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pwd"],"metadata":{"id":"h9opmgL1IroK"},"id":"h9opmgL1IroK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpath = imgfile = '/content/drive/MyDrive/Colab Notebooks/'\n","import os\n","os.listdir(gpath)"],"metadata":{"id":"PjOxj8GmJbo8"},"id":"PjOxj8GmJbo8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip show tensorflow"],"metadata":{"id":"5fHnTZ6ILkMA"},"id":"5fHnTZ6ILkMA","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorflow==2.14.0  # 2.12.0 - 2.14\n"],"metadata":{"id":"d5frfgwcLgrE"},"id":"d5frfgwcLgrE","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"be497c81-3b94-467a-afbd-638a37cd5f69","metadata":{"id":"be497c81-3b94-467a-afbd-638a37cd5f69"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import load_model\n","from PIL import Image, ImageOps\n","from IPython.display import Image, display\n","\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n","# from tensorflow.keras.preprocessing.image import array_to_img # load_img, img_to_array\n","\n","from tensorflow.image import rgb_to_grayscale\n","# from tensorflow.keras.utils import normalize"]},{"cell_type":"code","execution_count":null,"id":"3d3fc6dc-3ec9-44ba-88c1-7f107fa24c1d","metadata":{"id":"3d3fc6dc-3ec9-44ba-88c1-7f107fa24c1d"},"outputs":[],"source":["# model = load_model(gpath + 'Deep Learning Course/oxford_segmentation.h5') # from colab\n","model = load_model(gpath + 'Deep Learning Course/oxford_segmentation40.h5') # from colab\n","\n","# model.summary()\n","\n","input_shape = model.get_config()['layers'][0]['config']['batch_input_shape']\n","print(input_shape)\n","print('model input shape: ', model.get_config()['layers'][0]['config']['batch_input_shape'])"]},{"cell_type":"markdown","id":"6a16c769-4fe1-440c-986c-c50f03c049b6","metadata":{"id":"6a16c769-4fe1-440c-986c-c50f03c049b6"},"source":["# Load image and inference"]},{"cell_type":"code","execution_count":null,"id":"2d50def3-9ae6-42e7-9b4a-a57882ba91c0","metadata":{"id":"2d50def3-9ae6-42e7-9b4a-a57882ba91c0"},"outputs":[],"source":["in_shape = (160,160)\n","\n","# img = load_img('cat segment.jfif', target_size=in_shape) # ok\n","img = load_img(gpath + 'datasets/DL/cat615m.jpg', target_size=in_shape) # ok\n","# img = load_img(gpath + 'datasets/DL/bird1.jpg', target_size=in_shape) # ok\n","# img = load_img(gpath + 'datasets/DL/dog.4.jpg', target_size=in_shape) # ok\n","# img = load_img('cat segment.jfif', target_size=in_shape) # ok\n","# img = load_img('4.jpg', target_size=in_shape) # ok\n","\n","print('min-max pixel value=',np.min(img),np.max(img))  # not norm\n","img_test = np.expand_dims(img, axis=0)\n","print('shape=',img_test.shape)\n","\n","pred_mask = model.predict(img_test)[0]  # (160, 160, 3)\n","\n","mask = np.argmax(pred_mask, axis=-1)  # return (160,160) # pred_mask = pred_mask.reshape(160,160,3)  # Or this\n","mask = np.expand_dims(mask, axis=-1)  # (160, 160, 1) print(mask.shape)\n","\n","mask_img = ImageOps.autocontrast(array_to_img(mask))\n","# display(img) # display(mask_img)\n","display(img, mask_img)\n","display(array_to_img(pred_mask))"]},{"cell_type":"code","source":["# prompt: check unique values in mask using numpy\n","\n","unique_values = np.unique(mask_img2)\n","unique_values\n"],"metadata":{"id":"3JRZ27nA2pE_"},"id":"3JRZ27nA2pE_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# crop img\n","\n","import numpy as np\n","from PIL import Image\n","\n","# Create a new image with the same size as the original\n","cropped_img = Image.new(\"RGBA\", img.size)\n","\n","mask_img2 = array_to_img(mask) # Convert to grayscale\n","\n","for x in range(img.width):\n","    for y in range(img.height):\n","        if mask_img2.getpixel((x, y)) != 127: # Check for mask value\n","          cropped_img.putpixel((x, y), img.getpixel((x, y)))\n","\n","# Display the cropped image\n","display(cropped_img)\n"],"metadata":{"id":"fAHR_bEzaM7a"},"id":"fAHR_bEzaM7a","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: print min max of mask_img2\n","\n","print(f\"Min value of mask_img2: {np.min(np.array(mask_img2))}\")\n","print(f\"Max value of mask_img2: {np.max(np.array(mask_img2))}\")\n"],"metadata":{"id":"BfB7fhxGgE13"},"id":"BfB7fhxGgE13","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mask_img2"],"metadata":{"id":"ZdeLYRkMf6gV"},"id":"ZdeLYRkMf6gV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(mask, cmap='gray') # Use 'gray' colormap for a grayscale mask\n","plt.title('Segmentation Mask')\n","plt.show()\n"],"metadata":{"id":"ZsnfQ7m-fLRS"},"id":"ZsnfQ7m-fLRS","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1xk_KmSWciYu"},"id":"1xk_KmSWciYu","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}